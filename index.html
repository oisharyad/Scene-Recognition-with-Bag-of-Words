<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: uppercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Oisharya Dhar</h1>
</div>
</div>
<div class="container">

<h2> Assignment 4: Scene recognition with bag of words</h2>

<p align='justify'> This project deals with image recognition and classification.
	 This whole image recognition process in this project is divided into three types:<br>
	 <ol>
		<li>Tiny image and Nearest Neighbour Classifier</li>
		<li>Bag of Features and Nearest Neighbour Classifier</li>
		<li>Bag of Feature and SVM Classifier</li>
	</ol>
	 The dataset used in this assignment is divided into 15 categories.</p>
<p align='justify'>The first classifier and recognition model is tiny image representation and the nearest neighbour classifier. 
	For this firstly we will constract get_tiny_images and nearest_neighbor_classify functions.</p>
<p><h1>Get Tiny Features</h1></p>
<p align='justify'>This is the simplest step where each image of all categories is resized into 16*16-dimension images. For better performance, all the resized image is then normalized. 
	<br>For resizing cv2 function is used.
</p>
<pre><code>
#asiagning dimention for resizing
    height = 16
    width = 16
    dim = (height,width)

    #nested loop for taking all images in training and testing dataset
    for each_path in image_paths:
        each_image = load_image_gray(each_path)

        #resizing testing and training image 
        resize_image = cv2.resize(each_image, dim, interpolation = cv2.INTER_AREA)
        flatten_image = resize_image.flatten()

        #normalizing the resized images
        normalize_image = (flatten_image - np.mean(flatten_image))/np.std(flatten_image)
        feats.append(normalize_image)</code></pre>
<p align='justify'>Here, at first the dimention of the output image is determined. In this case it is 16*16. then a nested loop is run through all the images in every folder.
	A predefined function name <b>load_image_gray</b> is used to load every single image and by using OpenCV these images are resized. For normalizing purpose the rezised images are flatten.
</p>
<div style=" padding: 20px">
	<img src="../html/results/tiny.png"/>
	<p style="font-size: 20px" ><b>TINY IMAGE</b></p>

</div>

<p><h1>nearest_neighbor_classify</h1></p>
<p align='justify'>The nearest neighbour uses maximum patterns in the training image to predict the test dataset. 
	Here, pairwise distance is taken from the test image and train image.
	In this matter, as all the images are resized it takes the resized images and takes all the respective distance.
	Thus, by comparing the distance, this system can take the most familiar features.</p>
	<pre><code>#taking list of all calsses in training set
		classes = list(set(train_labels))
		k = 1
		dist = distance.cdist(train_image_feats, test_image_feats, 'euclidean')
	
		#nesteds for loop for generating the nearest distance
		for j in dist:
			no_label = []
	
			#taking all distance in accending order
			sort_dist = np.argsort(j)
			for i in range(k):
			no_label.append(train_labels[sort_dist[i]])
			
			#print(no_label)
			#taking the final category list
			for item in classes:
				if no_label.count(item) > 0:
					final_no_label = item
		
	 
			test_labels.append(final_no_label)</code></pre>
<p align='justify'>Here, K is the number of nearest neighbour. By using different value of K diffrerent accuracy is obtained.this can be shown in the table below,</p>
<table border=1 class="td_width"><tr>
	<th>value of k</th>
	<th>accuracy</th>
</tr>
<tr>
	<th>2</th>
	<th>21.87%</th>
</tr>
<tr>
	<th>1</th>
	<th>22.67%</th>
</tr>
<tr>
	<th>4</th>
	<th>19.60%</th>
</tr>
</table>
<P align='justify'>As the value of k increases the precesion gets lower. So, in this case k=1 is taken.</P>
<p align='justify'>Firstly, the eucladian distance of train and test image is taken. For generating nearest distance a nested for loop is used.
	By sorting every distance, the distances are put into a varible by accending order.
</p>
</div>
<p align='justify'>The accuracy using the tiny images and nearest neighbour classifier is 22.67%.
</p>	
<div style=" padding: 20px">
	<img src="../html/results/tiny-classi.png"/>
	<p style="font-size: 20px"><b>NN CLASSIFIER WITH TINY IMAGES</b></p>

</div>
<p align='justify'>The second classifier and recognition model is the bag of sifts and the nearest neighbour classifier. 
	To use bag of sift properfly we need to call a function name build_vocabulary./p>
<h1>Build Vocabulary</h1>
<p align='justify'>This function will help the sifts descriptor of the bag of sifts to sample training images and it will return cluster center or visual vocabulary with help of k-means.</p>
<pre><code>
    #nested for loop for taking features of all images
    for each_image in image_paths:
		img = load_image_gray(path)
    	frames, features = vlfeat.sift.dsift(img, step=[5,5], fast=True)
    	descriptor.append(features)

    #taking all the descriptor in one particular axis
    descriptor = np.concatenate(descriptor, axis=0).astype('float32')
    
	#kmeans clustering
    vocab = kmeans(descriptor, vocab_size, initialization="PLUSPLUS")        
 </code></pre>
<p align='justify'>Using dsift,all the frames and respective features are taken. After gathering all feature values in one list variable name <b>descriptor</b>, 
	the values are taken in one particular axis. From then, using kmeans clustering the vocabularies are found.</p>
<h1>Bag of sifts</h1>
<p align='justify'>This trains and tests images as a histogram. The main target is to count the number of 
	SIFT descriptor that falls under each vocab.</p>
	<pre><code>
		#asigning bin size for histogram
		bin=len(vocab)
		#for loop for generating histogram 
		for path in image_paths:
			image = load_image_gray(path)
			frames, descriptors = vlfeat.sift.dsift(image, step=[5,5], fast=True)
			dist = distance.cdist(vocab, descriptors, metric='euclidean')
			minimum_dist = np.argmin(dist, axis=0)
			hist, bin_edges = np.histogram(minimum_dist, bin)
			# normalizing histogram
			hist_norm = [float(i)/sum(hist) for i in hist]
			feats.append(hist_norm)
		
		feats = np.asarray(feats)</code></pre>
		<p align='justify'>As it uses histogram the bin size of histogram is declared. Same as vocabulary function the frames and descriptor vales are taken. Using the descriptores
			the smallest eucladian distance is taken. The histogram is generated by usning the minmum distance values.  </p>
<p align='justify'>By using bag of sifts and nearest neighbour classifier we get accuracy of 49.7%</p>
<div>
	<img src="../html/results/bag_nearest.png" />
	<p style= "text-align: center;" font-size: 20px>NN CLASSIFIER WITH BAG OF SIFTS</p>
</div>
<h1>Svm classifier</h1>
<p align='justify'>Here, a linear svm called one-vs-all is trained to operate in the bag of SIFT feature space. The operator for this can be found in,
	 <a href="url">https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</a><br> and the code for this,</p>
<pre><code>
SVM = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol= 1e-4, C=2000, multi_class='ovr', fit_intercept=True,
				intercept_scaling=1, class_weight=None, verbose=0, random_state=0,  
	  			max_iter= 2000)

#fitting the data to the model
SVM.fit(train_image_feats, train_labels)

#prediction of output
test_labels = SVM.predict(test_image_feats)</code></pre>
<p align='justify'>For SVM C is the Regularization Parameter. With trial and error the value of c is determined and for this c=2000 works efficiently. 
	After initializing the SVM, the trainings datas are being fed for the perdiction purposes.
</p>

<div>
		<img src="../html/results/bag-svm.png" />
		<p style= "text-align: center;" font-size: 20px>SVM CLASSIFIER WITH BAG OF SIFTS</p>
</div>

<h1>accuracy of SVM and nearest_neighbor_classify with different vocabulary number</h1>
<p align='justify'>I tried to run this code with vocabulary 1000 but my laptop crashes doing this and it takes over 950seconds.</p>
<style>
	.td_width td{
		width: 15%;
	}
	.td_width img{
		width: 100%;
	}
</style>
 <table border=1 class="td_width">
	<tr>
		<th>Vocabulary Size</th>
		<th>vocab=10</th>
		<th>vocab=20</th>
		<th>vocab=50</th>
		<th>vocab=100</th>
		<th>vocab=200</th>
		<th>vocab=400</th>
	</tr>
	<tr>
		<th>Time in second</th>
		<th>9.802</th>
		<th>22.19</th>
		<th>46.04</th>
		<th>88.63</th>
		<th>181.55</th>
		<th>364.23</th>
	</tr>
	<tr>
		<th>Nearest Neighbour Classifier</th>
		<td>
			<img src="../html/results/v=10-near.png"  />
		</td>
		<td>
			<img src="../html/results/v=20-near.png" />
		</td>
		<td>
			<img src="../html/results/v=50-near.png" />
		</td>
		<td>
			<img src="../html/results/v=100-near.png"  />
		</td>
		<td>
			<img src="../html/results/v=200-near.png" />
		</td>
		<td>
			<img src="../html/results/v=400-near.png" />
		</td>
	</tr>

	<tr>
		<th>SVM</th>
		<td>
			<img src="../html/results/v=10-svm.png"  />
		</td>
		<td>
			<img src="../html/results/v=20-svm.png" />
		</td>
		<td>
			<img src="../html/results/v-50-svm.png" />
		</td>
		<td>
			<img src="../html/results/v=100-svm.png"  />
		</td>
		<td>
			<img src="../html/results/v=200-svm.png" />
		</td>
		<td>
			<img src="../html/results/v=400-svm.png" />
		</td>
	
			
		
	</tr>

</table>

<h1>cross validation</h1>
<p align='justify'>As SVM classifier with bag of feature gives the highest accuracy among three,cross validation is used here. Random 100 images for test and 100 images for testing is taken from test and train dataset.
	Cross validation is used in svm classifier in 5 folds.The accuracy and standard deviation is given bewllow</p>
<table border=1 class="td_width"><tr>
	<th>Fold no</th>
	<th>accuracy</th>
</tr>
<tr>
	<th>1</th>
	<th>36.37%</th>
</tr>
<tr>
	<th>2</th>
	<th>35.41%</th>
</tr>
<tr>
	<th>3</th>
	<th>38.27%</th>
</tr>
<tr>
	<th>4</th>
	<th>30.98%%</th>
</tr>
<tr>
	<th>5</th>
	<th>38.21%</th>
</tr>
</table>
<p align='justify'>With this value we get,<br>
Standard Deviation=2.668<br>
Average Accuracy=35.85%</p>
<p align='justify'>This cross validation can be done for other two classifier as well.<br> For validation data, this assignment has already split the data in two types. the test data is used for validation.</p>

</body>
</html>
